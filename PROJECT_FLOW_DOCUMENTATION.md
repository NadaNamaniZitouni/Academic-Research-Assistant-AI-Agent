# Academic Research Assistant - Complete Project Flow Documentation

## Overview
This document provides a comprehensive, n8n-style workflow diagram of the entire project, showing how every file interacts and the complete data flow from user upload to query response.

---

## üèóÔ∏è Architecture Overview

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Frontend      ‚îÇ  React + Vite (Port 5173)
‚îÇ   (React App)   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ HTTP/REST API
         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Backend       ‚îÇ  FastAPI (Port 8010)
‚îÇ   (Python)      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚ñº         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Ollama ‚îÇ ‚îÇ FAISS  ‚îÇ
‚îÇ (LLM)  ‚îÇ ‚îÇ (Index)‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## üìã Table of Contents

1. [Frontend Flow](#frontend-flow)
2. [Backend Upload Flow](#backend-upload-flow)
3. [Backend Query Flow](#backend-query-flow)
4. [File-by-File Breakdown](#file-by-file-breakdown)
5. [Data Flow Diagrams](#data-flow-diagrams)
6. [Key Integration Points](#key-integration-points)

---

## üé® Frontend Flow

### Component: `main.jsx`
**Location:** `frontend/react-app/src/main.jsx`
**Purpose:** React application entry point
**Flow:**
```
main.jsx
  ‚îî‚îÄ> Renders <App /> component
  ‚îî‚îÄ> Imports index.css (global styles)
```

### Component: `App.jsx`
**Location:** `frontend/react-app/src/App.jsx`
**Purpose:** Main application container, manages upload/query state
**Flow:**
```
App.jsx
  ‚îú‚îÄ> State: hasUploaded (tracks if document uploaded)
  ‚îú‚îÄ> Renders Header (title + subtitle)
  ‚îú‚îÄ> Renders UploadPDF component
  ‚îÇ   ‚îî‚îÄ> onUploadComplete() callback ‚Üí sets hasUploaded = true
  ‚îî‚îÄ> Renders QueryInterface component
      ‚îî‚îÄ> Shows info box when hasUploaded = true
```

**Key Functions:**
- `handleUploadComplete()`: Called when upload succeeds, updates state

### Component: `UploadPDF.jsx`
**Location:** `frontend/react-app/src/components/UploadPDF.jsx`
**Purpose:** Handles PDF file upload UI and logic
**Flow:**
```
User selects file
  ‚îî‚îÄ> handleFileSelect()
      ‚îî‚îÄ> Validates file (PDF, size, not empty)
      ‚îî‚îÄ> Sets file state
      
User clicks "Upload"
  ‚îî‚îÄ> handleUpload()
      ‚îî‚îÄ> Calls api.uploadPDF(file, onProgress)
          ‚îî‚îÄ> api.js: uploadPDF()
              ‚îî‚îÄ> POST /upload (multipart/form-data)
                  ‚îî‚îÄ> Backend: main.py /upload endpoint
      
Upload Success
  ‚îî‚îÄ> onUploadComplete() callback
      ‚îî‚îÄ> Updates App.jsx state
      ‚îî‚îÄ> Shows success message
```

**Key Functions:**
- `handleFileSelect()`: Validates and stores selected file
- `handleUpload()`: Initiates upload via API
- `onUploadComplete()`: Notifies parent component

### Component: `QueryInterface.jsx`
**Location:** `frontend/react-app/src/components/QueryInterface.jsx`
**Purpose:** Handles query submission and result display
**Flow:**
```
User enters query
  ‚îî‚îÄ> handleSubmit()
      ‚îî‚îÄ> Calls api.query(queryText)
          ‚îî‚îÄ> api.js: query()
              ‚îî‚îÄ> POST /query (JSON)
                  ‚îî‚îÄ> Backend: main.py /query endpoint
      
Response received
  ‚îî‚îÄ> Displays:
      ‚îú‚îÄ> Answer (from LLM)
      ‚îú‚îÄ> Sources (chunks with citations)
      ‚îú‚îÄ> Related Papers
      ‚îî‚îÄ> Research Gaps
```

**Key Functions:**
- `handleSubmit()`: Submits query to backend
- `toggleChunk()`: Expands/collapses chunk details
- `fetchChunkDetails()`: Gets full chunk text for display

### Service: `api.js`
**Location:** `frontend/react-app/src/services/api.js`
**Purpose:** Centralized API client for backend communication
**Flow:**
```
api.js (Axios instance)
  ‚îú‚îÄ> Base URL: http://localhost:8010
  ‚îú‚îÄ> Timeout: 15 minutes
  ‚îú‚îÄ> Interceptors:
  ‚îÇ   ‚îú‚îÄ> Request: Handles FormData (removes Content-Type)
  ‚îÇ   ‚îî‚îÄ> Response: Logs responses
  ‚îÇ
  ‚îú‚îÄ> uploadPDF(file, onProgress)
  ‚îÇ   ‚îî‚îÄ> POST /upload
  ‚îÇ       ‚îî‚îÄ> FormData with file
  ‚îÇ       ‚îî‚îÄ> onUploadProgress callback
  ‚îÇ
  ‚îî‚îÄ> query(queryText)
      ‚îî‚îÄ> POST /query
          ‚îî‚îÄ> JSON: { query: string, k: number }
```

**Key Functions:**
- `uploadPDF()`: Handles file upload with progress tracking
- `query()`: Sends query request
- `getChunk()`: Fetches individual chunk details

---

## üì§ Backend Upload Flow

### Entry Point: `main.py` - `/upload` Endpoint
**Location:** `backend/app/main.py`
**Flow:**
```
POST /upload
  ‚îú‚îÄ> Receives: UploadFile (multipart/form-data)
  ‚îú‚îÄ> Validates: PDF file extension
  ‚îú‚îÄ> Generates: file_id (UUID)
  ‚îú‚îÄ> Saves file: UPLOAD_DIR / {file_id}.pdf
  ‚îî‚îÄ> Calls: ingest_pdf_with_embeddings(file_path, db)
      ‚îî‚îÄ> ingest.py: ingest_pdf_with_embeddings()
```

**Key Functions:**
- `upload_pdf()`: Main upload handler
- Returns: `{ doc_id, status, num_chunks }`

### Step 1: PDF Ingestion - `ingest.py`
**Location:** `backend/app/ingest.py`
**Flow:**
```
ingest_pdf_with_embeddings(pdf_path, db)
  ‚îÇ
  ‚îú‚îÄ> Step 1: Get FAISS Index
  ‚îÇ   ‚îî‚îÄ> faiss_index.py: get_faiss_index()
  ‚îÇ       ‚îî‚îÄ> Returns: FAISSIndex instance
  ‚îÇ       ‚îî‚îÄ> start_chunk_id = faiss_idx.get_total()
  ‚îÇ
  ‚îú‚îÄ> Step 2: Extract & Chunk PDF
  ‚îÇ   ‚îî‚îÄ> ingest_pdf(pdf_path, db, start_chunk_id)
  ‚îÇ       ‚îÇ
  ‚îÇ       ‚îú‚îÄ> pdf_extractor.py: extract_text_from_pdf()
  ‚îÇ       ‚îÇ   ‚îî‚îÄ> Extracts: full_text, metadata (title, authors, year, DOI)
  ‚îÇ       ‚îÇ
  ‚îÇ       ‚îú‚îÄ> pdf_extractor.py: extract_text_by_page()
  ‚îÇ       ‚îÇ   ‚îî‚îÄ> Returns: List of {page_num, text}
  ‚îÇ       ‚îÇ
  ‚îÇ       ‚îú‚îÄ> chunking.py: chunk_text_by_pages()
  ‚îÇ       ‚îÇ   ‚îî‚îÄ> Chunks text: size=800, overlap=200
  ‚îÇ       ‚îÇ   ‚îî‚îÄ> Returns: List of {text, page_start, page_end}
  ‚îÇ       ‚îÇ
  ‚îÇ       ‚îú‚îÄ> models.py: save_document_metadata()
  ‚îÇ       ‚îÇ   ‚îî‚îÄ> Creates: Document record in database
  ‚îÇ       ‚îÇ   ‚îî‚îÄ> Returns: doc_id (UUID)
  ‚îÇ       ‚îÇ
  ‚îÇ       ‚îî‚îÄ> models.py: ChunkMetadata
  ‚îÇ           ‚îî‚îÄ> Saves chunks to database
  ‚îÇ           ‚îî‚îÄ> Each chunk: chunk_id, doc_id, text, pages, etc.
  ‚îÇ
  ‚îú‚îÄ> Step 3: Generate Embeddings
  ‚îÇ   ‚îî‚îÄ> embeddings.py: get_embedding_service()
  ‚îÇ       ‚îî‚îÄ> Returns: EmbeddingService instance
  ‚îÇ       ‚îî‚îÄ> embedding_service.encode(chunk_texts)
  ‚îÇ           ‚îî‚îÄ> Uses: sentence-transformers/all-MiniLM-L6-v2
  ‚îÇ           ‚îî‚îÄ> Returns: numpy array (n_chunks √ó 384)
  ‚îÇ
  ‚îú‚îÄ> Step 4: Add to FAISS Index
  ‚îÇ   ‚îî‚îÄ> faiss_index.py: faiss_idx.add_vectors(embeddings)
  ‚îÇ       ‚îî‚îÄ> Normalizes vectors (L2)
  ‚îÇ       ‚îî‚îÄ> Adds to FAISS IndexFlatIP
  ‚îÇ       ‚îî‚îÄ> faiss_idx.save() ‚Üí saves to disk
  ‚îÇ
  ‚îî‚îÄ> Step 5: Add to Embedding Cache
      ‚îî‚îÄ> embedding_cache.py: get_embedding_cache()
          ‚îî‚îÄ> Returns: EmbeddingCache instance
          ‚îî‚îÄ> cache.add_embeddings(chunk_ids, embeddings)
              ‚îî‚îÄ> Stores in memory: {chunk_id: embedding}
              ‚îî‚îÄ> Saves to disk: embeddings_cache.npy + mapping.json
```

**Key Functions:**
- `ingest_pdf_with_embeddings()`: Main ingestion orchestrator
- `ingest_pdf()`: Extracts, chunks, and saves metadata
- Returns: `{ doc_id, chunks, num_chunks, embeddings_added }`

### Supporting Files in Upload Flow:

#### `pdf_extractor.py`
**Purpose:** PDF text and metadata extraction
**Key Functions:**
- `extract_text_from_pdf()`: Full text + metadata
- `extract_text_by_page()`: Page-by-page text

#### `chunking.py`
**Purpose:** Text chunking with overlap
**Key Functions:**
- `chunk_text_by_pages()`: Creates chunks with page info
- `chunk_text()`: Generic text chunking

#### `models.py`
**Purpose:** Database models (SQLAlchemy)
**Key Classes:**
- `Document`: Document metadata table
- `ChunkMetadata`: Chunk storage table

#### `database.py`
**Purpose:** Database connection and session management
**Key Functions:**
- `init_db()`: Creates tables
- `get_db()`: FastAPI dependency for DB sessions

#### `embeddings.py`
**Purpose:** Embedding generation service
**Key Classes:**
- `EmbeddingService`: Wraps SentenceTransformer
- `get_embedding_service()`: Singleton instance

#### `faiss_index.py`
**Purpose:** FAISS vector index management
**Key Classes:**
- `FAISSIndex`: Manages FAISS index operations
- `get_faiss_index()`: Singleton instance

#### `embedding_cache.py`
**Purpose:** Embedding cache for fast reranking
**Key Classes:**
- `EmbeddingCache`: In-memory + disk cache
- `get_embedding_cache()`: Singleton instance

---

## üîç Backend Query Flow

### Entry Point: `main.py` - `/query` Endpoint
**Location:** `backend/app/main.py`
**Flow:**
```
POST /query
  ‚îú‚îÄ> Receives: QueryRequest { query: str, k: int, doc_id: Optional[str] }
  ‚îú‚îÄ> Validates: query text not empty
  ‚îî‚îÄ> Calls: full_rag_pipeline(query_text, db, k=k, doc_id=doc_id)
      ‚îî‚îÄ> agents.py: full_rag_pipeline()
```

**Key Functions:**
- `query_endpoint()`: Main query handler
- Returns: `{ answer, sources, related_papers, gaps }`

### Step 1: Retrieve Chunks - `rag.py`
**Location:** `backend/app/rag.py`
**Flow:**
```
retrieve_chunks(query, db, k, doc_id=None)
  ‚îÇ
  ‚îú‚îÄ> Step 1.1: Encode Query
  ‚îÇ   ‚îî‚îÄ> embeddings.py: embedding_service.encode_query(query)
  ‚îÇ       ‚îî‚îÄ> Returns: query_embedding (384-dim vector)
  ‚îÇ
  ‚îú‚îÄ> Step 1.2: FAISS Search
  ‚îÇ   ‚îî‚îÄ> faiss_index.py: faiss_idx.search(query_embedding, initial_k=25)
  ‚îÇ       ‚îî‚îÄ> Returns: (distances, chunk_ids)
  ‚îÇ
  ‚îú‚îÄ> Step 1.3: Load Chunk Metadata
  ‚îÇ   ‚îî‚îÄ> database.py: db.query(ChunkMetadata)
  ‚îÇ       ‚îî‚îÄ> Filters by: chunk_id in results
  ‚îÇ       ‚îî‚îÄ> Filters by: doc_id (if specified) ‚ö†Ô∏è KEY FILTER
  ‚îÇ       ‚îî‚îÄ> Returns: List of chunk dicts
  ‚îÇ
  ‚îú‚îÄ> Step 1.4: Rerank Chunks
  ‚îÇ   ‚îî‚îÄ> rerank_chunks(query_embedding, chunks, cache)
  ‚îÇ       ‚îî‚îÄ> embedding_cache.py: cache.get_embeddings(chunk_ids)
  ‚îÇ       ‚îî‚îÄ> Computes: cosine similarity (query √ó chunks)
  ‚îÇ       ‚îî‚îÄ> Sorts: by similarity score
  ‚îÇ
  ‚îî‚îÄ> Step 1.5: MMR Diversity Selection
      ‚îî‚îÄ> mmr_diversity_selection(query_embedding, chunks, cache)
          ‚îî‚îÄ> Balances: relevance vs diversity
          ‚îî‚îÄ> Ensures: doc diversity (if doc_id=None)
          ‚îî‚îÄ> Returns: Top final_k chunks (default: 12)
```

**Key Functions:**
- `retrieve_chunks()`: Main retrieval orchestrator
- `rerank_chunks()`: Reranks using cached embeddings
- `mmr_diversity_selection()`: Selects diverse chunks
- `format_context_for_llm()`: Formats chunks for LLM prompt

### Step 2: Generate Answer - `agents.py`
**Location:** `backend/app/agents.py`
**Flow:**
```
answer_with_rag(chunks, question)
  ‚îÇ
  ‚îú‚îÄ> Format Context
  ‚îÇ   ‚îî‚îÄ> rag.py: format_context_for_llm(chunks, max_chunk_length=None)
  ‚îÇ       ‚îî‚îÄ> Creates: "Source 1 [Title, pX-Y]:\n{text}\n..."
  ‚îÇ
  ‚îú‚îÄ> Create Prompt
  ‚îÇ   ‚îî‚îÄ> PromptTemplate with:
  ‚îÇ       ‚îú‚îÄ> Context: Formatted chunks
  ‚îÇ       ‚îî‚îÄ> Question: User query
  ‚îÇ
  ‚îî‚îÄ> Call LLM
      ‚îî‚îÄ> llm_wrapper.py: get_llm_instance()
          ‚îî‚îÄ> Returns: LLM instance (Ollama/OpenAI/etc.)
          ‚îî‚îÄ> chain.run(context=context, question=question)
              ‚îî‚îÄ> LLM generates answer with citations
```

**Key Functions:**
- `answer_with_rag()`: Generates answer using RAG
- `get_llm_instance()`: Gets LLM singleton

### Step 3: Find Related Papers - `related_literature.py`
**Location:** `backend/app/related_literature.py`
**Flow:**
```
find_related_papers(query_text, answer_text, db)
  ‚îú‚îÄ> Combines: query + answer text
  ‚îú‚îÄ> keywords.py: extract_keywords() ‚Üí top 15 keywords
  ‚îú‚îÄ> embeddings.py: encode_query(keywords)
  ‚îú‚îÄ> faiss_index.py: search() ‚Üí top_k * 3 chunks
  ‚îú‚îÄ> Aggregates: by doc_id
  ‚îî‚îÄ> Returns: Top 5 related papers
```

### Step 4: Identify Gaps - `gap_analysis.py`
**Location:** `backend/app/gap_analysis.py`
**Flow:**
```
identify_research_gaps(answer, related_papers, question)
  ‚îú‚îÄ> Formats: related papers text
  ‚îú‚îÄ> Creates: PromptTemplate
  ‚îú‚îÄ> Calls: LLM with gap analysis prompt
  ‚îî‚îÄ> Parses: Gap descriptions and suggestions
```

### Complete Query Pipeline - `agents.py`
**Location:** `backend/app/agents.py`
**Flow:**
```
full_rag_pipeline(question, db, k, doc_id)
  ‚îú‚îÄ> Step 1: Retrieve Chunks
  ‚îÇ   ‚îî‚îÄ> rag.py: retrieve_chunks(question, db, k, doc_id)
  ‚îÇ
  ‚îú‚îÄ> Step 2: Generate Answer
  ‚îÇ   ‚îî‚îÄ> answer_with_rag(chunks, question)
  ‚îÇ
  ‚îú‚îÄ> Step 3: Find Related Papers
  ‚îÇ   ‚îî‚îÄ> related_literature.py: find_related_papers()
  ‚îÇ
  ‚îú‚îÄ> Step 4: Identify Gaps
  ‚îÇ   ‚îî‚îÄ> gap_analysis.py: identify_research_gaps()
  ‚îÇ
  ‚îî‚îÄ> Returns: {
        answer: str,
        sources: List[SourceResponse],
        related_papers: List[RelatedPaperResponse],
        gaps: List[GapResponse]
      }
```

---

## üìÅ File-by-File Breakdown

### Frontend Files

| File | Purpose | Key Exports/Functions |
|------|---------|----------------------|
| `main.jsx` | React entry point | Renders `<App />` |
| `App.jsx` | Main container | Manages upload/query state |
| `App.css` | App styles | Component styling |
| `index.css` | Global styles | Base styles, dark theme |
| `UploadPDF.jsx` | Upload component | File selection, upload logic |
| `QueryInterface.jsx` | Query component | Query submission, results display |
| `api.js` | API client | Axios instance, API functions |
| `vite.config.js` | Vite config | Dev server, proxy, HMR |

### Backend Files

| File | Purpose | Key Functions/Classes |
|------|---------|----------------------|
| `main.py` | FastAPI app | `/upload`, `/query` endpoints |
| `models.py` | Database models | `Document`, `ChunkMetadata` |
| `database.py` | DB connection | `init_db()`, `get_db()` |
| `schemas.py` | Pydantic schemas | `QueryRequest`, `QueryResponse` |
| `ingest.py` | PDF ingestion | `ingest_pdf_with_embeddings()` |
| `pdf_extractor.py` | PDF extraction | `extract_text_from_pdf()` |
| `chunking.py` | Text chunking | `chunk_text_by_pages()` |
| `embeddings.py` | Embedding service | `EmbeddingService`, `get_embedding_service()` |
| `faiss_index.py` | FAISS index | `FAISSIndex`, `get_faiss_index()` |
| `embedding_cache.py` | Embedding cache | `EmbeddingCache`, `get_embedding_cache()` |
| `rag.py` | RAG retrieval | `retrieve_chunks()`, `rerank_chunks()`, `mmr_diversity_selection()` |
| `agents.py` | RAG pipeline | `full_rag_pipeline()`, `answer_with_rag()` |
| `llm_wrapper.py` | LLM management | `get_llm()`, LLM provider setup |
| `related_literature.py` | Related papers | `find_related_papers()` |
| `gap_analysis.py` | Gap analysis | `identify_research_gaps()` |
| `keywords.py` | Keyword extraction | `extract_keywords()` |
| `index_manager.py` | Index management | `rebuild_index_from_database()` |
| `middleware.py` | Middleware | `setup_rate_limiting()` |
| `utils.py` | Utilities | Helper functions |

---

## üîÑ Data Flow Diagrams

### Upload Flow (Complete)
```
User (Browser)
  ‚îÇ
  ‚ñº
UploadPDF.jsx
  ‚îÇ handleUpload()
  ‚ñº
api.js: uploadPDF()
  ‚îÇ POST /upload (FormData)
  ‚ñº
main.py: /upload endpoint
  ‚îÇ Validates PDF
  ‚îÇ Saves file to disk
  ‚ñº
ingest.py: ingest_pdf_with_embeddings()
  ‚îÇ
  ‚îú‚îÄ> ingest_pdf()
  ‚îÇ   ‚îú‚îÄ> pdf_extractor.py: extract_text_from_pdf()
  ‚îÇ   ‚îú‚îÄ> pdf_extractor.py: extract_text_by_page()
  ‚îÇ   ‚îú‚îÄ> chunking.py: chunk_text_by_pages()
  ‚îÇ   ‚îú‚îÄ> models.py: save_document_metadata()
  ‚îÇ   ‚îî‚îÄ> models.py: ChunkMetadata (save chunks)
  ‚îÇ
  ‚îú‚îÄ> embeddings.py: encode()
  ‚îÇ   ‚îî‚îÄ> SentenceTransformer model
  ‚îÇ
  ‚îú‚îÄ> faiss_index.py: add_vectors()
  ‚îÇ   ‚îî‚îÄ> FAISS IndexFlatIP
  ‚îÇ
  ‚îî‚îÄ> embedding_cache.py: add_embeddings()
      ‚îî‚îÄ> Memory + Disk cache
  ‚îÇ
  ‚ñº
Response: { doc_id, status, num_chunks }
  ‚îÇ
  ‚ñº
UploadPDF.jsx: onUploadComplete()
  ‚îÇ
  ‚ñº
App.jsx: hasUploaded = true
```

### Query Flow (Complete)
```
User (Browser)
  ‚îÇ
  ‚ñº
QueryInterface.jsx
  ‚îÇ handleSubmit()
  ‚ñº
api.js: query()
  ‚îÇ POST /query (JSON)
  ‚ñº
main.py: /query endpoint
  ‚îÇ Validates request
  ‚ñº
agents.py: full_rag_pipeline()
  ‚îÇ
  ‚îú‚îÄ> Step 1: Retrieve Chunks
  ‚îÇ   ‚îî‚îÄ> rag.py: retrieve_chunks()
  ‚îÇ       ‚îú‚îÄ> embeddings.py: encode_query()
  ‚îÇ       ‚îú‚îÄ> faiss_index.py: search()
  ‚îÇ       ‚îú‚îÄ> database.py: query(ChunkMetadata)
  ‚îÇ       ‚îÇ   ‚îî‚îÄ> ‚ö†Ô∏è FILTER: doc_id (if specified)
  ‚îÇ       ‚îú‚îÄ> rag.py: rerank_chunks()
  ‚îÇ       ‚îÇ   ‚îî‚îÄ> embedding_cache.py: get_embeddings()
  ‚îÇ       ‚îî‚îÄ> rag.py: mmr_diversity_selection()
  ‚îÇ
  ‚îú‚îÄ> Step 2: Generate Answer
  ‚îÇ   ‚îî‚îÄ> agents.py: answer_with_rag()
  ‚îÇ       ‚îú‚îÄ> rag.py: format_context_for_llm()
  ‚îÇ       ‚îî‚îÄ> llm_wrapper.py: get_llm_instance()
  ‚îÇ           ‚îî‚îÄ> LLM (Ollama/OpenAI)
  ‚îÇ
  ‚îú‚îÄ> Step 3: Related Papers
  ‚îÇ   ‚îî‚îÄ> related_literature.py: find_related_papers()
  ‚îÇ
  ‚îî‚îÄ> Step 4: Research Gaps
      ‚îî‚îÄ> gap_analysis.py: identify_research_gaps()
  ‚îÇ
  ‚ñº
Response: { answer, sources, related_papers, gaps }
  ‚îÇ
  ‚ñº
QueryInterface.jsx: Display results
```

---

## üîó Key Integration Points

### 1. Document ID Filtering (‚ö†Ô∏è CRITICAL)
**Location:** `backend/app/rag.py` - `retrieve_chunks()`
**Issue:** When `doc_id` is provided, chunks are filtered:
```python
if doc_id is not None and chunk_meta.doc_id != doc_id:
    continue  # Skip chunks from other documents
```

**Problem:** This filter happens AFTER FAISS search, so:
- FAISS returns top 25 chunks (may include other documents)
- Filter removes chunks from other documents
- If top results are from wrong document, query fails

**Solution Needed:** Filter at FAISS level OR increase initial_k when doc_id specified

### 2. Embedding Cache Sync
**Location:** `backend/app/embedding_cache.py`
**Flow:**
- New document uploaded ‚Üí embeddings added to cache
- Cache stores: `{chunk_id: embedding}`
- Query uses cache for reranking

**Potential Issue:** If cache not synced, reranking uses wrong embeddings

### 3. Chunk ID Assignment
**Location:** `backend/app/ingest.py`
**Flow:**
```python
start_chunk_id = faiss_idx.get_total()  # Sequential IDs
chunk_id = start_chunk_id + idx
```

**Issue:** Chunk IDs are sequential across all documents, not per-document

### 4. FAISS Index
**Location:** `backend/app/faiss_index.py`
**Flow:**
- All document embeddings in single FAISS index
- Search returns top-k by similarity (across all docs)
- No built-in document filtering

**Issue:** FAISS doesn't know about doc_id, only chunk_id

---

## üêõ Potential Issues & Debugging Points

### Issue: Query Returns Wrong Document

**Check Points:**
1. **FAISS Search** (`rag.py:193`)
   - Does FAISS return chunks from correct document?
   - Check: `chunk_ids` from FAISS search

2. **Document Filter** (`rag.py:208`)
   - Is `doc_id` being passed correctly?
   - Check: `doc_id` parameter in `retrieve_chunks()`

3. **Chunk Metadata** (`rag.py:202`)
   - Are chunks loaded with correct `doc_id`?
   - Check: `chunk_meta.doc_id` matches expected

4. **Cache Embeddings** (`rag.py:229`)
   - Are cached embeddings for correct chunks?
   - Check: `cache.get_embeddings(chunk_ids)`

5. **MMR Selection** (`rag.py:234`)
   - Does MMR preserve document filter?
   - Check: Selected chunks still have correct `doc_id`

### Recommended Fix:
Increase `initial_k` when `doc_id` is specified to ensure enough chunks from target document:
```python
if doc_id is not None:
    initial_k = min(initial_k * 3, total_vectors)  # Get more candidates
```

---

## üìä Data Storage

### Database (SQLite)
**Location:** `data/research_assistant.db`
**Tables:**
- `documents`: Document metadata
- `chunk_metadata`: Chunk text and metadata

### FAISS Index
**Location:** `data/indices/faiss_index.idx`
**Content:** Vector embeddings (384-dim) for all chunks

### Embedding Cache
**Location:** `data/indices/embeddings_cache.npy` + `embeddings_cache.json`
**Content:** Embeddings + chunk_id mapping

### Uploaded Files
**Location:** `data/uploads/{file_id}.pdf`
**Content:** Original PDF files

---

## üîß Configuration

### Environment Variables
**Location:** `.env` (root) or `docker-compose.yml`
**Key Variables:**
- `LLM_PROVIDER`: ollama/openai/sambanova/gemini
- `LLM_MODEL_NAME`: mistral/gpt-3.5-turbo/etc.
- `OLLAMA_BASE_URL`: http://ollama:11434
- `EMBEDDING_MODEL`: sentence-transformers/all-MiniLM-L6-v2
- `DB_PATH`: Database file path
- `FAISS_INDEX_PATH`: FAISS index file path
- `UPLOAD_DIR`: Upload directory path

---

## üöÄ Docker Services

### Services (docker-compose.yml)
1. **ollama**: LLM service (port 11434)
2. **backend**: FastAPI app (port 8010)
3. **frontend**: React app (port 5173)

### Network
- All services on `research-network`
- Backend connects to Ollama via service name

---

## üìù Summary

This document provides a complete workflow view of the Academic Research Assistant project. The key insight is that **document filtering happens AFTER FAISS search**, which may cause queries to return chunks from wrong documents if the top FAISS results are from other documents.

**Recommended Fix:** Increase `initial_k` when `doc_id` is specified, or implement document-aware FAISS filtering.

